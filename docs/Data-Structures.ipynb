{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data structures\n",
    "\n",
    "## The mesh\n",
    "\n",
    "We make a distinction between the data structures we use for loading a mesh from disk, and the\n",
    "computational mesh (variables) that the program operates on. For MiniCombust, we do not prescribe\n",
    "the former (in fact, we recommend leveraging existing, tested implementations like libmesh that are\n",
    "compatible with gmsh).\n",
    "\n",
    "For the latter, we allow variation to test the effect of, say SoA vs AoS layout. PETSC-FUN3D\n",
    "\n",
    "In general, meshes can represent variables in terms of cells and faces, or in terms of nodes (vertices) and edges\n",
    "(links).\n",
    "It's possible to have staggered approaches where a variable of interest is defined at the cell centre,\n",
    "but flux variables are at nodes.\n",
    "\n",
    "For our problem, we use\n",
    "\n",
    "We consider only the 3D case for MiniCombust.\n",
    "\n",
    "Some common things are precalculated\n",
    "\n",
    "Faces have areas\n",
    "Cell have volumes\n",
    "\n",
    "\n",
    "## Representation for calculations\n",
    "[BASED ON dolfyn.90 READGEOMETRY]\n",
    "* For every cell we store its face (ids) and nodes [WHAT DO WE NEED NODAL VALUES FOR?]\n",
    "* For every face we store its adjacent cell (ids)\n",
    "\n",
    "\n",
    "Note that loading meshes, partitioning them and pre-calculating these properties of cells and volumes\n",
    "are all uninteresting from a scaling point of view: they're a once-off upfront cost. It's feasible that\n",
    "this would even be pre-processing before a 'run' of the simulation.  However, we measure the total \n",
    "up-front cost, in case this is smething that needd to be optimised.\n",
    "\n",
    "We don't even specify that these calculations needs to happen in parallel for MiniCombust.\n",
    "\n",
    "The long-running bit is the simulation.\n",
    "\n",
    "\n",
    "\n",
    "## Distributed meshes\n",
    "\n",
    "We use 'domain decomposition'\n",
    "\n",
    "### Partitioning\n",
    "Partitioning libraries like Parmetis partition graphs defined in terms of nodes and edges. \n",
    "The graph we want to partition in this case is not the same as the nodes and edges that make up the \n",
    "computational mesh. It's a description of how different cells are adjacent to each other.\n",
    "\n",
    "Here's an example:\n",
    "[SHOW A 3D MESH WITH CELLS AND FACES AND ITS CORRESPONDING GRAPH]\n",
    "\n",
    "### Representing ghost cells\n",
    "\n",
    "## Are the cells for the FVM the same as the cells for the particles?\n",
    "\n",
    "## Assembling a linear system from the mesh\n",
    "We'll use the industry standard PETSc to represent matrices and vectors for linear solver.\n",
    "Ideally we want zero-copy (DLPack)?\n",
    "\n",
    "How to go from mesh variables to the assembled matrix.\n",
    "Sparse matrix, dense vector.\n",
    "Possible for this to be matrix-free Jacobian, but maybe we just use PETSc.\n",
    "\n",
    "\n",
    "\n",
    "An unstructured mesh consists of polyhedral cells. We can define these by the edges (vertexes), cells and the\n",
    "faces (adjacent between two cells). See fig:\n",
    "\n",
    "[Fig 3.1 a from https://www.ctcms.nist.gov/fipy/download/fipy-1.2.3.pdf]\n",
    "\n",
    "\n",
    "https://scicomp.stackexchange.com/questions/4733/what-are-some-good-data-types-for-unstructured-cell-centered-fvm-cfd-code\n",
    "\n",
    "MiniCombust is face-based, i.e. Cell Centered FVM (CC-FVM)\n",
    "\n",
    "We want a distributed mesh where we can find the neighbouring faces\n",
    "\n",
    "??\n",
    "\n",
    "In FVM, we are concerned with the average value of a variable of interest within a control volume (i.e. cell). Thiis\n",
    "average value is stored at the cell centre. Fluxes for a face are approximated using the values from the two cells adjacent to the face. This is a low-order approximation, but is simple to store and results in systems with low-bandwidth matrices.\n",
    "\n",
    "We use co-located rather than staggered grids for pressure and velocity/heat?\n",
    "\n",
    "Mesh elements (cells, faces, edges) have Globally unique IDs (over distributed memories)\n",
    "Each element may also have a local ID, that is not globally unqiue.\n",
    "This allows renumbering locally for more efficient data structure access.\n",
    "Cells are grouped into locally owned an ghost cells\n",
    "\n",
    "\n",
    "The computational mesh consists of polyhedral cells, the faces of which have maximum 4 vertices\n",
    "\n",
    "Face connectivity is also stored as CSR for easy traversal.\n",
    "\n",
    "Vertex connectivity connectivity is stored as a distributed adjacency list:\n",
    "\n",
    "CSR?\n",
    " CSR xadj  vs adjncy\n",
    "For each node we know whether it is local or ghost\n",
    "For ghost nodes, we know the process it belongs to\n",
    "\n",
    "Since is store as CSR, we can always trivially look up neighbour nodes of a cell based on it's position in CSR\n",
    "Describe\n",
    "\n",
    "We partitional the cells by building a graph where nodes are the cell ID and edges define adjaccent cells\n",
    "\n",
    "\n",
    "Nodes are totally a thing. We rely on a nodes/edges thing to partition mesh and also what does CSR mean if not\n",
    "nodes and edges.\n",
    "\n",
    "(we use process ID rather than rank, but you can think of it as the same fr MPI)\n",
    "\n",
    "In contrast to NetworkX (edges, vertices), meshes are defined by their faces.\n",
    "IS THIS TRUE?\n",
    "\n",
    "The canonical app uses libmesh or something instead of implementing this from scratch?\n",
    "\n",
    "\n",
    "Inspired by the PETSC-FUN3D papers (https://www.mcs.anl.gov/research/projects/petsc-fun3d/Papers/sc00.pdf) we\n",
    "use AoS (\"Interlacing\"), CSR, Structural Blocking, Edge Reordering (Reverse Cuthill-McKee)\n",
    "But actually want MPI-OpenMP (Maybe?)\n",
    "\n",
    "## Partitioning the mesh over nodes\n",
    "Uses Parmetis to partition the mesh for a number of ranks.\n",
    "Use ParMetis \n",
    "Using a partitioner such as parmetis or metis, the nodes of the mesh are partitioned \n",
    "What does this mean for faces and cells?\n",
    "Who owns a face variable?\n",
    "\n",
    "\n",
    "## Sparse matrices\n",
    "Each timestep results in a linear system Ax = b where A is sparse.\n",
    "How do we represent these?\n",
    "Are we matrix free?\n",
    "\n",
    "## Timesteps, iterations and sweeps\n",
    "Timestep: delta T as system evolves\n",
    "Sweeps: we improve non-linear PDE accuracy by running several sweeps of the solution for each timestep, subsituting the\n",
    "previous solution value at the same time step value\n",
    "Iterations: number of iterations of linear solver\n",
    "\n",
    "\n",
    "## Boundary type\n",
    "Inlet, Outlet, wall, symmety planes?\n",
    "\n",
    "\n",
    "```Fortran\n",
    "////\n",
    " type :: CellData\n",
    "     real    :: x(3)                      ! center coordinates\n",
    "     real    :: vol                       ! volume\n",
    "     integer :: ctid = 1                  ! fluid type id as set in fluid table\n",
    "   end type\n",
    "\n",
    "///\n",
    "```\n",
    "\n",
    "\n",
    "## Particles\n",
    "```Fortran\n",
    "   type ParticleProp\n",
    "     real    :: x0(3) = 0.0                        ! starting coordinates\n",
    "     real    :: v0(3) = 0.0                        ! starting velocity\n",
    "     real    :: x1(3) = 0.0                        ! current coordinates\n",
    "     real    :: v1(3) = 0.0                        ! current velocity\n",
    "     real    :: a1(3) = 0.0                        ! current acceleration\n",
    "     integer :: cell0 =  0                         ! starting cell the particle is in\n",
    "     integer :: cell1 =  0                         ! current cell the particle is in\n",
    "     integer :: face  =  0                         ! current face the particle is on\n",
    "     real    :: dens0 = -1.0  ! <= undefined flag  ! current density\n",
    "     real    :: diam0 =  1.0\n",
    "     real    :: mass0 =  1.0\n",
    "     logical :: wall  = .false.\n",
    "   end type\n",
    "```\n",
    "\n",
    "\n",
    "https://www.cfd-online.com/Wiki/Discretization_of_the_diffusion_term\n",
    "\n",
    "\n",
    "```\n",
    "// Read number of cells\n",
    "read ScaleFactor [detail, ignore]\n",
    "allocate the Cell array (each has 3d coords, vol and fluid type id)\n",
    "\n",
    "\n",
    "for each cell\n",
    "    fill in coords and vol, fuel type\n",
    "apply scale factor to all geom components and volume\n",
    "\n",
    "read maxfaces [maximum facces of any cell]\n",
    "allocate NFaces array (int number of faces for each cell)\n",
    "allocate CFace array (list of faces)\n",
    "\n",
    "```\n",
    "\n",
    "### Questions\n",
    "Does each processor need to kmnow more that its global-local mappings (i.e. nobody needs the total mapping?)\n",
    "\n",
    "In the design, can we use a master/workers approach so that the partitioning etc. is done in the master node (maybe in parallel?) and then the workers are started with their bits, already renumbered?\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}