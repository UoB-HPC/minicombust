{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Interprocess communication in MiniCombust\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Halo Exchange for Cells\n",
    "\n",
    "The unstructured mesh is distributed through the memories of several processes (MPI ranks).\n",
    "Each rank only needs to store the part of the mesh it will be operating on, plus some surrounding cells\n",
    "required for computation. Neighbouring processes will need to exchange the values of border cells to\n",
    "keep values up to ate. \n",
    "\n",
    "In this section, we look at this process in detail.\n",
    "\n",
    "We have an unstructured mesh of cells, in which each MPI rank has several neighbour ranks.\n",
    "\n",
    "Some of the cells are interior to the MPI rank: no other rank will need their values to be able to\n",
    "calculate the fluid fields. Howeverm some of the cells are \"border cells\", and neighbouring ranks\n",
    "need to know the values in these cells for their calculations. In addition, this rank needs to \n",
    "know values of the adjacent cells to its border cells, which belong to neighbouring ranks: its stores\n",
    "these \"ghost cells\", but never updates their values.\n",
    "\n",
    "At the end of each timestep calculation, we perform a _halo exchange_ to tell neighbours the new\n",
    "values of our boundary cells, and to receive new values for our \"ghost cells\".\n",
    "\n",
    "Fortunately, after the partitioning is complete, each process knows statically:\n",
    "* Which are its neighbour ranks\n",
    "* Which cells are border cells, and which neighbour ranks need their values\n",
    "* Which cells are ghost cells, and which neighbour ranks to receive the updated values from\n",
    "\n",
    "Recall that each process stores its cells (whether interior, boundary and ghost cells) in a \n",
    "densely packed, contiguous local structure. Each cell has a local id, which identifies the\n",
    "cell in this structure. However, each cell also has a global id, which is the same accross all ranks.\n",
    "Consequently, we store maps that allow local processes to look up the global id for a local cell and vice versa.\n",
    "\n",
    "After partitioning, each process know which global cell ids to send to which neighbour, and which \n",
    "global cell ids to receive from each neighbour. We can precompute some structures:\n",
    "\n",
    "* A receive buffer, which knows how much data we'll be receiving from each neighbour, and which\n",
    "local cell that represents. This allos us to easily perform an optimised _local scatter_ operation to\n",
    "redistribute these received values from the receive buffer back to their correct position in the local cells array.\n",
    "* Since we know which global cell ids we'll be sending to which neighbour, we can build a densely\n",
    "packed send buffer and perform an optimised _local gather_ operation, which \n",
    "collects the values we want to send from their positions in the local cells array and places them\n",
    "in the densely packed send structure.\n",
    "\n",
    "![An unstructured mesh distributed over 4 processes](images/halo-exchange.svg \"Halo exchange for a distributed mesh\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "source": [
    "from mpi4py import MPI\n",
    "from typing import Iterable, Dict, List\n",
    "from numpy.typing import NDArray\n",
    "from minicombust.data_structures import T, LocalId, GlobalId\n",
    "import numpy as np\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "my_rank = comm.Get_rank()\n",
    "nproc = comm.Get_size()\n",
    "print('Hello from rank {}'.format(my_rank))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello from rank 0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "# In lieu of partitioning and a real mesh, we'll use this simple example\n",
    "# from the image in the docs\n",
    "\n",
    "# Who each ranks neighbours are\n",
    "mesh_neighbour_ranks = {\n",
    "    0:[1],\n",
    "    1:[0,2],\n",
    "    2:[1,3],\n",
    "    3:[2]\n",
    "}\n",
    "\n",
    "# How many of a rank's ghost cells are owned by each neighbour \n",
    "mesh_neighbour_rank_to_num_ghost_cells = {\n",
    "    0: {1:2},\n",
    "    1: {0:2, 2:2},\n",
    "    2: {1:2, 3:2},\n",
    "    3: {2:2}\n",
    "}\n",
    "\n",
    "# How many of a rank's border cells are held as ghost cells by each neighbour \n",
    "mesh_neighbour_rank_to_num_border_cells = {\n",
    "    0 : {1:2},\n",
    "    1 : {0:2, 2:2},\n",
    "    2 : {1:2, 3:2},\n",
    "    3 : {2:2}\n",
    "}\n",
    "\n",
    "# Which global cell ids must be received from which neighbour, for each rank \n",
    "mesh_neighbour_rank_to_ghost_global_cell_ids = {\n",
    "    0: {1: [2,3]},\n",
    "    1: {0: [0,1], 2: [4,5]},\n",
    "    2: {1: [2,3], 3: [6,7]},\n",
    "    3: {2: [4,5]}\n",
    "}\n",
    "\n",
    "# Which global cell ids must be sent to which neighbour, for each rank\n",
    "mesh_neighbour_rank_to_border_global_cell_ids = {\n",
    "    0: {1: [0,1]},\n",
    "    1: {0: [2,3], 2: [2,3]},\n",
    "    2: {1: [4,5], 3: [4,5]},\n",
    "    3: {2: [6,7]}\n",
    "}\n",
    "\n",
    "# map from global cell id for local cell id for each rank (only bother with border cells and ghost cells)\n",
    "mesh_global_cell_to_local_cell_id = {\n",
    "    0: {0:0, 1:1, 2:2, 3:3},\n",
    "    1: {0:0, 1:1, 2:2, 3:3, 4:4, 5:5},\n",
    "    2: {2:0, 3:1, 4:2, 5:3, 6:4, 7:5},\n",
    "    3: {4:0, 5:1, 6:2, 7:3}\n",
    "}\n",
    "\n",
    "# map from local cell id for global cell id for each rank (only bother with border cells and ghost cells)\n",
    "mesh_local_cell_to_global_cell_id = {\n",
    "    0: {0:0, 1:1, 2:2, 3:3},\n",
    "    1: {0:0, 1:1, 2:2, 3:3, 4:4, 5:5},\n",
    "    2: {0:2, 1:3, 2:4, 3:5, 4:6, 5:7},\n",
    "    3: {0:4, 1:5, 2:6, 3:7}\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%%capture\n",
    "# This is all of the mesh that each local process needs to store:\n",
    "neighbour_ranks: List[int] = mesh_neighbour_ranks[my_rank]\n",
    "neighbour_rank_to_num_ghost_cells: Dict[int, np.int64] = mesh_neighbour_rank_to_num_ghost_cells[my_rank]\n",
    "neighbour_rank_to_num_boundary_cells: Dict[int, np.int64] = mesh_neighbour_rank_to_num_border_cells[my_rank]\n",
    "neighbour_rank_to_boundary_global_cell_ids = mesh_neighbour_rank_to_border_global_cell_ids[my_rank]\n",
    "neighbour_rank_to_ghost_global_cell_ids = mesh_neighbour_rank_to_ghost_global_cell_ids[my_rank]\n",
    "global_cell_to_local_cell_id: Dict[GlobalId, LocalId] = mesh_global_cell_to_local_cell_id[my_rank]\n",
    "local_cell_to_global_cell_id: Dict[LocalId, GlobalId] = mesh_local_cell_to_global_cell_id[my_rank]\n",
    "neighbour_rank_to_received_cell_values: Dict[int, NDArray] = {i: np.zeros((len(neighbour_rank_to_ghost_global_cell_ids[i])), dtype=T) for i in neighbour_ranks}\n",
    "cells: NDArray[T] = np.zeros((len(local_cell_to_global_cell_id),), dtype=T)\n",
    "\n",
    "# Just some dummy values for this example\n",
    "cells[:] = my_rank\n",
    "\n",
    "def gather_local_cell_values_for_ids(ids : Iterable[LocalId]) -> NDArray[T]:\n",
    "    return np.take_along_axis(cells, np.array(list(ids)), axis=None)\n",
    "\n",
    "def send_to_single_neighbour(dst_rank: int):\n",
    "    global_cell_ids = neighbour_rank_to_boundary_global_cell_ids[dst_rank]\n",
    "    local_cell_ids = (global_cell_to_local_cell_id[i] for i in global_cell_ids)\n",
    "    src_cell_vals = gather_local_cell_values_for_ids(local_cell_ids)\n",
    "    #print(\"{} sends border to {}: {}\".format(my_rank, dst_rank, len(src_cell_vals)))\n",
    "    return comm.Isend(src_cell_vals, dest=dst_rank)\n",
    "\n",
    "def receive_from_single_neighbour(src_rank: int):\n",
    "    arr = neighbour_rank_to_received_cell_values[src_rank]\n",
    "    #print(\"{} receives ghost from {}: {}\".format(my_rank, src_rank, len(arr)))\n",
    "    return comm.Irecv(arr, source=src_rank)\n",
    "\n",
    "def perform_updates_on_local_cells():\n",
    "    # Insert fluid solver of your choice here\n",
    "    pass\n",
    "\n",
    "def scatter_received_halos_to_cells():\n",
    "    def scatter_single_rank(rank):       \n",
    "        global_cell_ids = neighbour_rank_to_ghost_global_cell_ids[rank]\n",
    "        local_cell_ids = np.array([global_cell_to_local_cell_id[i] for i in global_cell_ids])\n",
    "        np.put_along_axis(cells, local_cell_ids, neighbour_rank_to_received_cell_values[rank], axis=None)\n",
    "\n",
    "    for rank in neighbour_ranks:\n",
    "        scatter_single_rank(rank)\n",
    "\n",
    "\n",
    "def halo_exchange():\n",
    "    # We use non-blocking comms for the exchange\n",
    "    # to avoid deadlock. Dependencies are mesh-dependent\n",
    "    # and so are difficult to create perfect blocking comms for.\n",
    "    # But this might be worth optimising more\n",
    "    reqs = []\n",
    "    for rank in range(len(neighbour_ranks)):\n",
    "        reqs.append(receive_from_single_neighbour(neighbour_ranks[rank]))\n",
    "        reqs.append(send_to_single_neighbour(neighbour_ranks[rank]))\n",
    "    MPI.Request.Waitall(reqs)\n",
    "    scatter_received_halos_to_cells()\n",
    "\n",
    "def main_loop():\n",
    "    halo_exchange()\n",
    "    for timestep, idx in enumerate(range(10)):\n",
    "        perform_updates_on_local_cells()\n",
    "        halo_exchange()\n",
    "        if my_rank == 0:\n",
    "            print(\"Timestep {} complete\".format(idx))\n",
    "\n",
    "def main():\n",
    "    main_loop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'mesh_neighbour_ranks' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2489a2acf273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is all of the mesh that each local process needs to store:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mneighbour_ranks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh_neighbour_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mneighbour_rank_to_num_ghost_cells\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh_neighbour_rank_to_num_ghost_cells\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mneighbour_rank_to_num_boundary_cells\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh_neighbour_rank_to_num_border_cells\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mneighbour_rank_to_boundary_global_cell_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmesh_neighbour_rank_to_border_global_cell_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmy_rank\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mesh_neighbour_ranks' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exchange of particles that leave the region of cells owned by a rank"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Distributed particle exchange](images/particle-exchange.svg \"Distributed particle exchange \")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ExchangeParticles:\n",
    "\n",
    "We'll already have marked each particle with its owning rank\n",
    "group particles by rank\n",
    "for each neighbour:\n",
    "  exchange expected particle counts with neighbours\n",
    "wait(all)\n",
    "for each neighbour:\n",
    "    if > 0:\n",
    "    isend particles\n",
    "    irecv particles\n",
    "wait(all)\n",
    "\n",
    "When do we exchange?\n",
    "Every time \n",
    "We proceed when nobody has anything left to send (I won't be receiving or sending any particles)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Communication for Particles\n",
    "\n",
    "In MiniCombust, Lagrangian particle tracking is decomposed over distributed nodes. \n",
    "Each particle tracking rank only has a subset of the particles in the system.\n",
    "\n",
    "Particle tracking ranks are reponsible for \n",
    "* Adjusting the position, velocity, diameter, mass etc. of particles based on their Lagrangian motion\n",
    "* Adjusting particle properties based on the fluid field (e.g. contribution of surrounding field velocity to particle motion)\n",
    "* Adjusting the fluid field based on particles (e.g. evaporating particles that become a source term for the gas flow)\n",
    "\n",
    "\n",
    "In addition the communication required at the end of each fluid solver timestep to exchange the halos (cell values),\n",
    "particle tracking ranks may need to move particles that are out of their domain.\n",
    "\n",
    "During the Lagrangian particle update, a particle may still have \"travel time\" left over, but intersects a face that means ownership\n",
    "of the particle should pass to another rank.\n",
    "\n",
    "Each rank knows which cells are ghost cells\n",
    "For ghosts cells, we know which neighbour rank that belongs to\n",
    "We need to transfer the particle to the neighbour rank\n",
    "\n",
    "If particles don't interact with each other (Do they in PRECISE?):\n",
    "Reach a point where each particle has been updated 'as much as it can in this timestep'\n",
    "Tell each neighbour a list of particles (potentially empty)\n",
    "Tell each neighbour how many particles are coming first and then send?\n",
    "\n",
    "LOAD IMBALANCE when most particles clustered in particular cell (likely)\n",
    "\n",
    "Can we a dynamic process / executor approach to update each particle irrespective of whether its cell is on rank?\n",
    "i.e. we  gather all its cell stuff and send it for processing\n",
    "As a result of processing it can be adddedd back to processing pool (still time left, but cell adjustments to be made)\n",
    "\n",
    "What does this mean for local particle ids? Need to mark inactive and periodically defrag???\n",
    "Remove particle from structures? Should particle structures be more dynamic and based on map? Code is difficult to vectorise.\n",
    "\n",
    "\n",
    "The number of particles can change several times in the course of a single fluid timestep!\n",
    "\n",
    "Fluid timesteps are much bigger than particle timesteps? Or is it the other way round? Think it might be the other way round? How often does fluid update vs particle?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('jupyter-cHpSGFXA': pipenv)"
  },
  "interpreter": {
   "hash": "a956620d842dc2e525c827335584ec63975fbc6e2437e3993664d6579b484a03"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}